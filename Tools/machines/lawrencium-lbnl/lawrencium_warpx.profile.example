# please set your project account
#export proj="<yourProject>"  # change me, e.g., ac_blast

# required dependencies
module load cmake/3.24.1
module load cuda/11.4
module load gcc/7.4.0
module load openmpi/4.0.1-gcc

# optional: for QED support with detailed tables
module load boost/1.70.0-gcc

# optional: for openPMD and PSATD+RZ support
module load hdf5/1.10.5-gcc-p
module load lapack/3.8.0-gcc
# CPU only:
#module load fftw/3.3.8-gcc

export CMAKE_PREFIX_PATH=$HOME/sw/v100/c-blosc-1.21.1:$CMAKE_PREFIX_PATH
export CMAKE_PREFIX_PATH=$HOME/sw/v100/adios2-2.8.3:$CMAKE_PREFIX_PATH
export CMAKE_PREFIX_PATH=$HOME/sw/v100/blaspp-master:$CMAKE_PREFIX_PATH
export CMAKE_PREFIX_PATH=$HOME/sw/v100/lapackpp-master:$CMAKE_PREFIX_PATH

# optional: CCache
#module load ccache  # missing

# optional: for Python bindings or libEnsemble
module load python/3.8.8

if [ -d "$HOME/sw/v100/venvs/warpx" ]
then
  source $HOME/sw/v100/venvs/warpx/bin/activate
fi

# an alias to request an interactive batch node for one hour
#   for parallel execution, start on the batch node: srun <command>
alias getNode="salloc -N 1 -t 1:00:00 --qos=es_debug --partition=es1 --constraint=es1_v100 --gres=gpu:1 --cpus-per-task=4 -A $proj"
# an alias to run a command on a batch node for up to 30min
#   usage: runNode <command>
alias runNode="srun -N 1 -t 1:00:00 --qos=es_debug --partition=es1 --constraint=es1_v100 --gres=gpu:1 --cpus-per-task=4 -A $proj"

# optimize CUDA compilation for 1080 Ti (deprecated)
#export AMREX_CUDA_ARCH=6.1
# optimize CUDA compilation for V100
export AMREX_CUDA_ARCH=7.0
# optimize CUDA compilation for 2080 Ti
#export AMREX_CUDA_ARCH=7.5

# compiler environment hints
export CXX=$(which g++)
export CC=$(which gcc)
export FC=$(which gfortran)
export CUDACXX=$(which nvcc)
export CUDAHOSTCXX=${CXX}
