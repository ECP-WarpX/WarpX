# please set your project account
export proj="<yourProject>_g"  # change me

# load E4S & Spack
module load e4s/22.05

mySWenvDir="/global/common/software/${proj%_g}/$USER/spack-22.05-envs/warpx-perlmutter-cuda"
if [ ! -d "${mySWenvDir}" ]
  spack env create -d ${mySWenvDir} spack-perlmutter-cuda.yaml
fi
spack env activate -d ${mySWenvDir}

# an alias to request an interactive batch node for one hour
#   for parallel execution, start on the batch node: srun <command>
alias getNode="salloc -N 1 --ntasks-per-node=4 -t 1:00:00 -q interactive -C gpu --gpu-bind=single:1 -c 32 -G 4 -A $proj"
# an alias to run a command on a batch node for up to 30min
#   usage: runNode <command>
alias runNode="srun -N 1 --ntasks-per-node=4 -t 0:30:00 -q interactive -C gpu --gpu-bind=single:1 -c 32 -G 4 -A $proj"

# necessary to use CUDA-Aware MPI and run a job
export CRAY_ACCEL_TARGET=nvidia80

# optimize CUDA compilation for A100
export AMREX_CUDA_ARCH=8.0

# optimize CPU microarchitecture for AMD EPYC 3rd Gen (Milan/Zen3)
# note: the cc/CC/ftn wrappers below add those
#export CXXFLAGS="-march=znver3"
#export CFLAGS="-march=znver3"

# compiler environment hints
export CC=$(which gcc)
export CXX=$(which g++)
export FC=$(which gfortran)
export CUDACXX=$(which nvcc)
export CUDAHOSTCXX=${CXX}
